{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from tensorflow.keras.preprocessing.\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import namedtuple\n",
    "import lmdb"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
    "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')\n",
    "data_dir = \"/Users/nicolacave/dsi_galvanize/capstones/capstone3/handwriting_reader/data\"\n",
    "f = open(data_dir + \"/\" + 'gt/words.txt')\n",
    "chars = set()\n",
    "bad_samples_reference = ['a01-117-05-02', 'r06-022-03-05']  # known broken images in IAM dataset\n",
    "for line in f:\n",
    "    # ignore comment line\n",
    "    if not line or line[0] == '#':\n",
    "        continue\n",
    "\n",
    "    line_split = line.strip().split(' ')\n",
    "    assert len(line_split) >= 9\n",
    "\n",
    "    # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "    file_name_split = line_split[0].split('-')\n",
    "    file_name_subdir1 = file_name_split[0]\n",
    "    file_name_subdir2 = f'{file_name_split[0]}-{file_name_split[1]}'\n",
    "    file_base_name = line_split[0] + '.png'\n",
    "    file_name = data_dir + \"/\" + 'img' + \"/\" + file_name_subdir1 + \"/\" + file_name_subdir2 + \"/\" + file_base_name\n",
    "\n",
    "    if line_split[0] in bad_samples_reference:\n",
    "        print('Ignoring known broken image:', file_name)\n",
    "        continue\n",
    "\n",
    "    # GT text are columns starting at 9\n",
    "    gt_text = ' '.join(line_split[8:])\n",
    "    chars = chars.union(set(list(gt_text)))\n",
    "\n",
    "    # put sample into list\n",
    "    samples = []\n",
    "    samples.append(Sample(gt_text, file_name))\n",
    "    \n",
    "\n",
    "# split into training and validation set: 95% - 5%\n",
    "data_split = 0.95\n",
    "split_idx = int(data_split * len(samples))\n",
    "train_samples = samples[:split_idx]\n",
    "validation_samples = samples[split_idx:]\n",
    "\n",
    "# put words into lists\n",
    "train_words = [x.gt_text for x in train_samples]\n",
    "validation_words = [x.gt_text for x in validation_samples]\n",
    "\n",
    "# start with train set\n",
    "#train_set()\n",
    "\n",
    "# list of all chars in dataset\n",
    "char_list = sorted(list(chars))\n",
    "print(samples)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Ignoring known broken image: /Users/nicolacave/dsi_galvanize/capstones/capstone3/handwriting_reader/data/img/a01/a01-117/a01-117-05-02.png\n",
      "Ignoring known broken image: /Users/nicolacave/dsi_galvanize/capstones/capstone3/handwriting_reader/data/img/r06/r06-022/r06-022-03-05.png\n",
      "[Sample(gt_text='?', file_path='/Users/nicolacave/dsi_galvanize/capstones/capstone3/handwriting_reader/data/img/r06/r06-143/r06-143-04-10.png')]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(train_words)\n",
    "print(train_samples)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# pd.read_csv('../data/gt/words.txt', delimiter=\" \", skiprows=18, header=None)\n",
    "# data.columns = [\"seq\", \"date\", \"Hour\", \"GMT\",\"userID\",\"text\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATADIR = \"/Users/nicolacave/dsi_galvanize/capstones/capstone3/handwriting_reader/data/img\"\n",
    "\n",
    "CATEGORIES = [i for i in os.listdir(DATADIR) if not i == \".DS_Store\"]\n",
    "print(CATEGORIES)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# DATADIR = \"/Users/nicolacave/dsi_galvanize/capstones/capstone3/handwriting_reader/data/img\"\n",
    "\n",
    "# CATEGORIES = [i for i in os.listdir(DATADIR) if not i == \".DS_Store\"]\n",
    "# print(CATEGORIES)\n",
    "\n",
    "for category in CATEGORIES:  # do dogs and cats\n",
    "    path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "    #print(path)\n",
    "    #if os.path.isdir(path):\n",
    "    for folder in os.listdir(path):\n",
    "        #print(f\"folder: {folder}\")\n",
    "        for img in os.listdir(path+\"/\"+folder):\n",
    "            #print(f\"img: {img}\")\n",
    "            #print(path+\"/\"+folder+\"/\"+img)\n",
    "            #if not img.startswith('.'):\n",
    "            img_array = cv2.imread(os.path.join(path,folder,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                #print(img_array)\n",
    "            #plt.imshow(img_array, cmap='gray')  # graph it\n",
    "            #plt.show()  # display!\n",
    "\n",
    "        #break  # we just want one for now so break\n",
    "    #break  #...and one more!"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "training_data = []\n",
    "#print(len(CATEGORIES))\n",
    "def create_training_data():\n",
    "    print(len(CATEGORIES)*0.8)\n",
    "    for category in CATEGORIES[:int(len(CATEGORIES)*0.8)]:  # do dogs and cats\n",
    "        path = os.path.join(DATADIR,category)  # create path to dogs and cats\n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0 or a 1). 0=dog 1=cat\n",
    "        for folder in os.listdir(path):\n",
    "            for img in os.listdir(path+\"/\"+folder):\n",
    "        #for img in tqdm(os.listdir(path)):  # iterate over each image per dogs and cats\n",
    "                try:\n",
    "                    img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                    new_array = cv2.resize(img_array, (128, 128), interpolation=cv2.INTER_AREA)  # resize to normalize data size\n",
    "                    training_data.append([new_array, class_num])  # add this to our training_data\n",
    "                except Exception as e:  # in the interest in keeping the output clean...\n",
    "                    print(e)\n",
    "                    #pass\n",
    "                #except OSError as e:\n",
    "                #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "                #except Exception as e:\n",
    "                #    print(\"general exception\", e, os.path.join(path,img))\n",
    "\n",
    "create_training_data()\n",
    "\n",
    "\n",
    "print(len(training_data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(training_data))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\",input_shape=[137,236,1]),#input shape: [rows, columns, channels]\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.fit(training_data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "24bd0a8d74f35437157cf36f14598ef3477f99df4cd340566b47027f8059ba3f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}